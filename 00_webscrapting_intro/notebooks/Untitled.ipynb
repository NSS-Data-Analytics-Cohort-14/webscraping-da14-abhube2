{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a21e2f04-4567-406b-b3d6-b770696370b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://realpython.github.io/fake-jobs/'\n",
    "resp = requests.get(url)\n",
    "resp.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0312f2b0-6ea8-4035-8618-5a52e3168045",
   "metadata": {},
   "source": [
    "1a. Find the first job title\n",
    "We know job titles live in an <h2> with class title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddaa20ff-ecb1-4760-898e-4d870e3b94ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senior Python Developer\n"
     ]
    }
   ],
   "source": [
    "first_title_tag = soup.find('h2', class_='title')\n",
    "first_title     = first_title_tag.get_text(strip=True)\n",
    "print(first_title)\n",
    "# → \"Senior Python Developer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e65ec5-81c4-45a7-90fd-08e221ce4467",
   "metadata": {},
   "source": [
    "1b. Extract all job titles into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e1da97e-7ad0-4535-8038-66f41207ec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Senior Python Developer', 'Energy engineer', 'Legal executive', 'Fitness centre manager', 'Product manager']\n"
     ]
    }
   ],
   "source": [
    "title_tags = soup.find_all('h2', class_='title')\n",
    "titles     = [tag.get_text(strip=True) for tag in title_tags]\n",
    "print(titles[:5])  # show first five"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756e79e-ed14-40ae-9f38-31a16844cee9",
   "metadata": {},
   "source": [
    "1c. Extract companies, locations, and posting dates\n",
    "\n",
    "On this page:\n",
    "\n",
    "Company is in <h3> with class company\n",
    "Location is in <p> with class location\n",
    "Date is in <time> (use its datetime attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bdc3443-0e61-41f5-be52-42bbecad0810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payne, Roberts and Davis Stewartbury, AA 2021-04-08\n"
     ]
    }
   ],
   "source": [
    "company_tags  = soup.find_all('h3', class_='company')\n",
    "location_tags = soup.find_all('p', class_='location')\n",
    "time_tags     = soup.find_all('time')\n",
    "\n",
    "companies = [tag.get_text(strip=True) for tag in company_tags]\n",
    "locations = [tag.get_text(strip=True) for tag in location_tags]\n",
    "dates     = [tag['datetime'] for tag in time_tags]\n",
    "\n",
    "# quick sanity check\n",
    "print(companies[0], locations[0], dates[0])\n",
    "# → Payne, Roberts and Davis Stewartbury, AA 2021-04-08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4077f27-7352-4d2f-9fad-b89a6a0ef92e",
   "metadata": {},
   "source": [
    "1d. Combine into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b4fb759-ffcd-403c-b1ed-c36911926cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     title                     company              location  \\\n",
      "0  Senior Python Developer    Payne, Roberts and Davis       Stewartbury, AA   \n",
      "1          Energy engineer            Vasquez-Davidson  Christopherville, AA   \n",
      "2          Legal executive  Jackson, Chambers and Levy   Port Ericaburgh, AA   \n",
      "3   Fitness centre manager              Savage-Bradley     East Seanview, AP   \n",
      "4          Product manager                 Ramirez Inc   North Jamieview, AP   \n",
      "\n",
      "  date_posted  \n",
      "0  2021-04-08  \n",
      "1  2021-04-08  \n",
      "2  2021-04-08  \n",
      "3  2021-04-08  \n",
      "4  2021-04-08  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'title':       titles,\n",
    "    'company':     companies,\n",
    "    'location':    locations,\n",
    "    'date_posted': dates\n",
    "})\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f41931aa-6989-4826-ac94-9a613e4aa16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "base_url   = 'https://realpython.github.io/fake-jobs/'\n",
    "# assume you already have: job_cards = soup.find_all('div', class_='card')\n",
    "# and df has one row per card\n",
    "\n",
    "# Method A: loop over each card\n",
    "apply_urls = [\n",
    "    urljoin(base_url,\n",
    "            card.find('a', class_='card-footer-item')['href'])\n",
    "    for card in job_cards\n",
    "]\n",
    "\n",
    "df['apply_url_bs'] = apply_urls\n",
    "\n",
    "# Quick sanity check:\n",
    "print(len(df), len(apply_urls))\n",
    "# should both print 100 (or however many cards you have)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "593938d0-2e87-4937-b7f6-903f2c2f006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Software Engineer (Python) → software-engineer-python\n",
      "Scientist, research (maths) → scientist-research-maths\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "base_url = 'https://realpython.github.io/fake-jobs/'\n",
    "\n",
    "def make_slug(title: str) -> str:\n",
    "    s = title.lower()\n",
    "    s = s.replace('&', 'and')                    # “&” → “and”\n",
    "    s = re.sub(r'[()\\[\\],/?:!\\'\"]+', '', s)       # strip punctuation\n",
    "    s = re.sub(r'[^a-z0-9]+', '-', s)             # non-alnum → dash\n",
    "    return s.strip('-')\n",
    "\n",
    "# 1) turn each title into its “raw” slug\n",
    "raw_slugs = [make_slug(t) for t in titles]\n",
    "\n",
    "# 2) assign the -0, -1, … suffix based on first/second occurrence\n",
    "counts = {}\n",
    "constructed_hrefs = []\n",
    "for slug in raw_slugs:\n",
    "    n = counts.get(slug, 0)\n",
    "    counts[slug] = n + 1\n",
    "    constructed_hrefs.append(f'jobs/{slug}-{n}.html')\n",
    "\n",
    "# 3) build absolute URLs\n",
    "apply_urls_calc = [urljoin(base_url, href) for href in constructed_hrefs]\n",
    "\n",
    "# 4) stick ’em in the DataFrame\n",
    "df['apply_url_calc'] = apply_urls_calc\n",
    "\n",
    "# sanity-check: these should all be True\n",
    "print((df['apply_url_bs'] == df['apply_url_calc']).all())\n",
    "\n",
    "# example: inspect the two tricky titles\n",
    "for t in [\"Software Engineer (Python)\", \"Scientist, research (maths)\"]:\n",
    "    print(t, \"→\", make_slug(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f173f39-7b4b-4ea9-b4fb-027929b47734",
   "metadata": {},
   "source": [
    "Number 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79ca85cb-6c12-4ff2-a73f-6af398380be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professional asset web application environmentally friendly detail-oriented asset. Coordinate educational dashboard agile employ growth opportunity. Company programs CSS explore role. Html educational grit web application. Oversea SCRUM talented support. Web Application fast-growing communities inclusive programs job CSS. Css discussions growth opportunity explore open-minded oversee. Css Python environmentally friendly collaborate inclusive role. Django no experience oversee dashboard environmentally friendly willing to learn programs. Programs open-minded programs asset.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_next_sibling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 2b) Method 2: find the company header, then grab its very next <p> sibling\u001b[39;00m\n\u001b[0;32m     17\u001b[0m company_h3      \u001b[38;5;241m=\u001b[39m job_soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubtitle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m first_paragraph \u001b[38;5;241m=\u001b[39m company_h3\u001b[38;5;241m.\u001b[39mfind_next_sibling(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(first_paragraph)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_next_sibling'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1) Fetch & parse the Senior Python Developer page\n",
    "job_url  = 'https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html'\n",
    "resp     = requests.get(job_url)\n",
    "resp.raise_for_status()\n",
    "job_soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "# 2a) Method 1: grab the first <p> inside the main “content” container\n",
    "content_div     = job_soup.find('div', class_='content')\n",
    "first_paragraph = content_div.find('p').get_text(strip=True)\n",
    "print(first_paragraph)\n",
    "\n",
    "\n",
    "# 2b) Method 2: find the company header, then grab its very next <p> sibling\n",
    "company_h3      = job_soup.find('h3', class_='subtitle')\n",
    "first_paragraph = company_h3.find_next_sibling('p').get_text(strip=True)\n",
    "print(first_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fdbf844-fe3b-4152-a8b4-4bfb98df1e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At be than always different American address. Former claim chance prevent why measure too. Almost before some military outside baby interview. Face top individual win suddenly. Parent do ten after those scientist. Medical effort assume teacher wall. Significant his himself clearly very. Expert stop area along individual. Three own bank recognize special good along.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_job_description(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the given fake-jobs URL and returns the first\n",
    "    paragraph of the job description (stripped of extra whitespace).\n",
    "    \"\"\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    # the description lives in the first <p> inside <div class=\"content\">\n",
    "    content_div = soup.find('div', class_='content')\n",
    "    if not content_div:\n",
    "        return ''\n",
    "    p = content_div.find('p')\n",
    "    return p.get_text(strip=True) if p else ''\n",
    "\n",
    "# Example\n",
    "url = \"https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html\"\n",
    "description = get_job_description(url)\n",
    "print(description)\n",
    "# → \"At be than always different American address. Former claim chance prevent why measure too. Almost before some military outside baby interview. Face top individual win suddenly. Parent do ten after those scientist. Medical effort assume teacher wall. Significant his himself clearly very. Expert stop area along individual. Three own bank recognize special good along.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e656b1e-0841-405d-b368-f38a9d9c4f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
